name: scraper
on:
  push:
    branches:
      - master
  schedule:
    - cron:  '0 */15 * * *'

jobs:
  build:
    runs-on: ubuntu-latest
    steps:

      - name: checkout repo content
        uses: actions/checkout@v2 # checkout the repository content to github runner

      - name: setup python
        uses: actions/setup-python@v2
        with:
          python-version: '3.7.7' # install the python version needed
          
      - name: install python packages
        run: |
          python -m pip install --upgrade pip
          python -m pip install --upgrade pip
          pip install -U pip setuptools
          pip install pendulum beautifulsoup4 requests
      - name: Script check
        uses: jannekem/run-python-script-action@v1
        with:
          script: |
            import requests
            import os
            import json
            import time
            import csv
            import urllib
            from datetime import datetime
            from bs4 import BeautifulSoup
            jpath='hello.csv'
            now = datetime.now()
            infile=os.path.join(jpath,'catalog.json')

            idrun=[]
            def tdiff():
                # Get last modification time for file
                modTimesinceEpoc = os.path.getmtime(infile)
                modificationTime = time.strftime('%Y-%m-%d', time.localtime(modTimesinceEpoc))
                return modificationTime==now.strftime('%Y-%m-%d')

            def ulink(asset_id):
                asset_uid = asset_id.replace('/', '_')
                asset_url = "https://developers.google.com/earth-engine/datasets/catalog/{}".format(
                    asset_uid)
                thumbnail_url = 'https://mw1.google.com/ges/dd/images/{}_sample.png'.format(
                    asset_uid)

                #print(thumbnail_url)

                r = requests.get(thumbnail_url)

                try:
                    if r.status_code != 200:
                        html_page = urllib.request.urlopen(asset_url)
                        soup = BeautifulSoup(html_page, features="html.parser")

                        for img in soup.findAll('img'):
                            if 'sample.png' in img.get('src'):
                                thumbnail_url = img.get('src')
                                return [asset_url,thumbnail_url]

                    return [asset_url,thumbnail_url]
                except Exception as e:
                    print(e)

            def parseurl(url,outname):
                gparse= []
                try:
                    response=requests.get(url)
                    if response.status_code==200:
                        r=response.json()
                        gee_id=r['id']
                        gee_title=r['title']
                        gee_type=r['gee:type']
                        gee_start=r['extent']['temporal']['interval'][0][0].split('T')[0]
                        if not r['extent']['temporal']['interval'][0][1]==None:
                            gee_end=r['extent']['temporal']['interval'][0][1].split('T')[0]
                        else:
                            gee_end=now.strftime('%Y-%m-%d')
                        gee_start_year=gee_start.split('-')[0]
                        gee_end_year=gee_end.split('-')[0]
                        gee_provider=r['providers'][0]['name']
                        gee_tags=r['keywords']
                        asset_url,thumbnail_url=ulink(gee_id)
                        idrun.append(gee_id)
                        print('Processed a total of {} assets'.format(len(idrun)), end='\r')
                        with open(outname,'a') as csvfile:
                            writer=csv.writer(csvfile,delimiter=',',lineterminator='\n')
                            writer.writerow([gee_id,gee_provider,gee_title,gee_start,gee_end,gee_start_year,gee_end_year,gee_type,', '.join(gee_tags),asset_url,thumbnail_url])
                        csvfile.close()
                except Exception as e:
                    print(e)

            def ee_catalog():
                outname=os.path.join('hello.csv')
                if os.path.exists(infile):
                    with open(outname,'w') as csvfile:
                        writer=csv.DictWriter(csvfile,fieldnames=["id", "provider", "title", "start_date","end_date", "startyear","endyear","type","tags","asset_url","thumbnail_url"], delimiter=',',lineterminator='\n')
                        writer.writeheader()
                    obj = requests.get('https://earthengine-stac.storage.googleapis.com/catalog/catalog.json').json()
                    try:
                        for assets in obj['links']:
                            if assets['rel']=='child':
                                parseurl(assets['href'],outname)
                    except Exception as e:
                        print(e)
            ee_catalog()
      - name: execute py script # run sj-gobierno.py to get the latest data
        run: python ee_data_stac.py
      - name: file_check
        run: ls -l -a
      - name: commit files
        run: |
          git add -A
          git commit -m "update data" -a
      - name: push changes
        uses: ad-m/github-push-action@v0.6.0
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: main
